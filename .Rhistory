```{r, fig.width=20, fig.height=10, dpi=45}
d = data
# Selection based on Moisl 2011
select = selection(d, z = 1.645)
select = select[,4]
# Normalisations
d = relativeFreqs(d)
# save data for robustness checks
d = d[select,]
POS3grSave = d
d = normalisations(d)
myCAH = cluster::agnes(t(d), metric = "manhattan", method="ward")
# Save
CAHPOS3gr = myCAH
#TODO: heights
# barplot(sort(myCAH$height))
plotPOS3grams = cahPlotCol(myCAH, k = 5, main = "POS 3-grams (Transkr/Boudams/Pie/Pie)")
somCAH = somCluster(d)
somCAHPOS3gr = somCAH
somplotPOS3grams = cahPlotCol(somCAH, k = 5, main = "SOM BASED - POS 3-grams")
```
## Lemmas
```{r}
data = read.csv("data/transkr_lemmas.csv", header = TRUE, row.names = 1, sep = ";")
#remove total freq
data = data[, -1]
colnames(data) = gsub("^X", "", colnames(data))
colnames(data) = gsub(".decolumnized", "", colnames(data))
colnames(data) = gsub("Leg.", "Leg-", colnames(data))
data = data[, toKeep]
data = data[rowSums(data) > 0, ]
data = as.matrix(data)
```
### Burrows + vector-length norm
```{r, fig.width=20, fig.height=10, dpi=45}
d = data
# Selection based on Moisl 2011
select = selection(d, z = 1.645)
select = select[,4]
# Normalisations
d = relativeFreqs(d)
d = d[select,]
LemmasSave = d
d = normalisations(d)
myCAH = cluster::agnes(t(d), metric = "manhattan", method="ward")
# Save
CAHLemmas = myCAH
#TODO: heights
# barplot(sort(myCAH$height))
plotLemmas = cahPlotCol(myCAH, k = 5, main = "Lemmas (Transkr/Boudams/Pie/Pie)")
somCAH = somCluster(d)
somCAHLemmas = somCAH
somplotLemmas = cahPlotCol(somCAH, k = 5, main = "SOM BASED - Lemmas")
```
## Function words from lemmas
```{r}
# Find function words
#rownames(data)[1:250]
functionLemmas = source("functionLemmas.R")$value
```
### Burrows + vector-length norm
```{r, fig.width=20, fig.height=10, dpi=45}
d = relativeFreqs(data)
d = d[functionLemmas,]
FLSave = d
d = normalisations(d)
myCAH = cluster::agnes(t(d), metric = "manhattan", method="ward")
# Save
CAHFL = myCAH
# barplot(sort(myCAH$height))
plotFL = cahPlotCol(myCAH, k = 5, main = "Function Lemmas with pronouns and auxiliaries\n(Transkr/Boudams/Pie)")
#plotCol(myCAH, main = "toto")
somCAH = somCluster(d)
somCAHFL = somCAH
somplotFL = cahPlotCol(somCAH, k = 5, main = "SOM BASED - Function words (lemmas)")
```
# Affixes + POS 3-gr + Function words (lemmas)
```{r}
data = rbind(AffixesSave, POS3grSave, FLSave)
```
```{r}
d = normalisations(data)
myCAH = cluster::agnes(t(d), metric = "manhattan", method="ward")
# Save
CAHGlob = myCAH
#TODO: heights
# barplot(sort(myCAH$height))
plotGlob = cahPlotCol(myCAH, k = 5, main = "Affixes + POS 3- grams + Function words (lemmas)")
somCAH = somCluster(d)
somCAHGlob = somCAH
somplotGlob = cahPlotCol(somCAH, k = 5, main = "SOM BASED - Affixes + POS 3- grams + Function words (lemmas)")
```
# Affixes + POS 3-gr + Function words (unnorm)
```{r}
data = rbind(AffixesSave, POS3grSave, FWSave)
```
```{r}
d = normalisations(data)
myCAH = cluster::agnes(t(d), metric = "manhattan", method="ward")
# Save
CAHGlob2 = myCAH
#TODO: heights
# barplot(sort(myCAH$height))
plotGlob2 = cahPlotCol(myCAH, k = 5, main = "Affixes + POS 3- grams + Function words (unnorm.)")
somCAH = somCluster(d)
somCAHGlob2 = somCAH
somplotGlob2 = cahPlotCol(somCAH, k = 5, main = "SOM BASED - Affixes + POS 3- grams + Function words (unnorm.)")
```
# Affixes + POS 3-gr + Function words (both)
```{r}
data = rbind(AffixesSave, POS3grSave, FWSave, FLSave)
```
```{r}
d = normalisations(data)
myCAH = cluster::agnes(t(d), metric = "manhattan", method="ward")
# Save
CAHGlob3 = myCAH
#TODO: heights
# barplot(sort(myCAH$height))
plotGlob3 = cahPlotCol(myCAH, k = 5, main = "Affixes + POS 3- grams + Function words (both)")
somCAH = somCluster(d)
somCAHGlob3 = somCAH
somplotGlob3 = cahPlotCol(somCAH, k = 5, main = "SOM BASED - Affixes + POS 3- grams + Function words (unnorm.)")
```
# Exp. word-forms+lemmas
```{r}
data = rbind(LemmasSave, WordsSave)
```
```{r}
d = normalisations(data)
myCAH = cluster::agnes(t(d), metric = "manhattan", method="ward")
# Save
CAHWordsLemmas = myCAH
#TODO: heights
# barplot(sort(myCAH$height))
plotWordsLemmas = cahPlotCol(myCAH, k = 5, main = "Word forms + lemmas")
somCAH = somCluster(d)
somCAHWordsLemmas = somCAH
somplotWordsLemmas = cahPlotCol(somCAH, k = 5, main = "SOM BASED - Word forms + lemmas")
```
# Plots and tables
## Reference results on the three feature sets
```{r, warning=FALSE, fig.width=14.6, fig.height=21.9, out.width=1000, out.height=1500, dpi = 100}
gridExtra::grid.arrange(plotRaw3grams, plotGlob2, plotWordsLemmas, ncol = 1)
```
## Complementary results
```{r, warning=FALSE, fig.width=14.6, fig.height=21.9, out.width=1000, out.height=1500, dpi = 100}
#featlabel = "features of ME ±2σ with conf. > 90%"
#A = cahPlotCol(CAHLemma, main = "A", xlab = paste( ncol(CAHLemma$data), featlabel), k = 6, lrect = -12)
# B = cahPlotCol(CAHRhyme, main = "B", xlab = paste( ncol(CAHRhyme$data), featlabel), k = 6, lrect = -7, ylab = " ")
# C = cahPlotCol(CAHAllWords, main = "C", xlab = paste( ncol(CAHAllWords$data), featlabel), k = 6, ylab = " ")
# D = cahPlotCol(CAHAffs, main = "D", xlab = paste( ncol(CAHAffs$data), featlabel), k = 6, ylab = " ")
# E = cahPlotCol(CAHPOS3gr, main = "E", xlab = paste( ncol(CAHPOS3gr$data), featlabel), k = 6, lrect = -12 , ylab = " ")
# F = cahPlotCol(CAHmfw, main = "F", k = 6, lrect = -5, ylab = " ")
# gridExtra::grid.arrange(A, B, C, D, E, F, ncol = 2)
gridExtra::grid.arrange(plotAffixes, plotFW, plotFL, plotPOS3grams, plotForms, plotLemmas, ncol = 2)
```
## Comparison three alternatives, FS2
```{r}
gridExtra::grid.arrange(plotGlob, plotGlob2, plotGlob3, ncol = 1)
```
<!-- ## Analyses -->
<!-- ```{r, warning=FALSE, fig.width=14.6, fig.height=21.9, out.width=1000, out.height=1500, dpi = 100} -->
<!-- #featlabel = "features of ME ±2σ with conf. > 90%" -->
<!-- #A = cahPlotCol(CAHLemma, main = "A", xlab = paste( ncol(CAHLemma$data), featlabel), k = 6, lrect = -12) -->
<!-- # B = cahPlotCol(CAHRhyme, main = "B", xlab = paste( ncol(CAHRhyme$data), featlabel), k = 6, lrect = -7, ylab = " ") -->
<!-- # C = cahPlotCol(CAHAllWords, main = "C", xlab = paste( ncol(CAHAllWords$data), featlabel), k = 6, ylab = " ") -->
<!-- # D = cahPlotCol(CAHAffs, main = "D", xlab = paste( ncol(CAHAffs$data), featlabel), k = 6, ylab = " ") -->
<!-- # E = cahPlotCol(CAHPOS3gr, main = "E", xlab = paste( ncol(CAHPOS3gr$data), featlabel), k = 6, lrect = -12 , ylab = " ") -->
<!-- # F = cahPlotCol(CAHmfw, main = "F", k = 6, lrect = -5, ylab = " ") -->
<!-- # gridExtra::grid.arrange(A, B, C, D, E, F, ncol = 2) -->
<!-- gridExtra::grid.arrange(plotRaw3grams, plotForms, plotAffixes, plotFW, plotLemmas, plotFL, plotPOS3grams, plotGlob, ncol = 2) -->
<!-- ``` -->
<!-- ```{r, warning=FALSE, fig.width=14.6, fig.height=21.9, out.width=1000, out.height=1500, dpi = 100} -->
<!-- gridExtra::grid.arrange(somplotRaw3grams, somplotForms, somplotAffixes, somplotFW, somplotLemmas, somplotFL, somplotPOS3grams, somplotGlob, ncol = 2) -->
<!-- ``` -->
## Robustness
```{r}
cahList = list(raw3grams = CAHRaw3gr, Affs = CAHAffs, FunctWords = CAHFW, FunctLemm = CAHFL, POS3gr = CAHPOS3gr, FWPOSandAffs = CAHGlob2, Forms = CAHForms,  Lemmas = CAHLemmas, WordsLemmas = CAHWordsLemmas)
#compareHC(cahList, k = 5)
benchmark = benchmarkHC(CAHRaw3gr, cahList, k = 5)
round(benchmark, digits = 2)
# # Now with SOM
# cahSOMList = list(raw3grams = somCAHRaw3gr, Affs = somCAHAffs, FunctLemm = somCAHFL, POS3gr = somCAHPOS3gr, FLPOSandAffs = somCAHGlob, FWPOSandAffs = somCAHGlob2, FLFWPOSandAffs = somCAHGlob3, Forms = somCAHForms,  Lemmas = somCAHLemmas, WordsLemmas = somCAHWordsLemmas, UnnormFW = somCAHFW)
#
# benchmark = benchmarkHC(CAHRaw3gr, cahSOMList, k = 5)
# round(benchmark, digits = 2)
```
## Volatility index
## Ref analyses
```{r}
# ONLY on the three reference analyses
cahList = list(raw3grams = CAHRaw3gr,  FWPOSandAffs = CAHGlob2, WordsLemmas = CAHWordsLemmas)
vol = volatility(cahList, k = 5)
volRef = merge(round(vol, digits = 2), nwords, by="row.names", all.x=TRUE, all.y=FALSE)
volRef[order(volRef[, "V_i"]), ]
# see if there is a correlation
reg = lm(volRef[, 3] ~ volRef[, 2])
summary(reg)
plot(volRef[, 2], volRef[, 3])
abline(reg)
# Et la distrib des VI
boxplot(volRef[, 2])
hist(volRef[, 2])
```
## Supplementary analyses
```{r}
# ONLY on the three reference analyses
cahList = list(Affs = CAHAffs, FunctWords = CAHFW, FunctLemm = CAHFL, POS3gr = CAHPOS3gr, Forms = CAHForms,  Lemmas = CAHLemmas)
vol = volatility(cahList, k = 5)
volSuppl = merge(round(vol, digits = 2), nwords, by="row.names", all.x=TRUE, all.y=FALSE)
volSuppl[order(volSuppl[, "V_i"]), ]
# see if there is a correlation
reg = lm(volSuppl[, 3] ~ volSuppl[, 2])
summary(reg)
plot(volSuppl[, 2], volSuppl[, 3])
abline(reg)
# Et la distrib des VI
boxplot(volSuppl[, 2])
hist(volSuppl[, 2])
```
## Formatted table with both
```{r}
out = merge(volRef, volSuppl, by="row.names", all.x=TRUE, all.y=TRUE)
rownames(out) = out[, 2]
out = out[, c(4, 3, 6)]
colnames(out) = c("NWords", "V_iRef", "V_iSuppl")
out[order(out[, 2]),]
```
```{r}
# First, transform data
volRegr = rbind( data.frame(NWords = volRef[, 3], V_i = volRef[, 2], type = "Ref"),
data.frame(NWords = volSuppl[, 3], V_i = volSuppl[, 2], type = "Suppl"))
library(ggpmisc)
ggplot(volRegr, aes(NWords, V_i, shape=type, colour=type, fill=type)) + geom_smooth(method="lm") +
geom_point(size=3) + theme_bw() +
# ggpmisc::stat_poly_eq(formula = quote(V_i) ~ quote(NWords), aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), parse = TRUE)
ggpmisc::stat_fit_glance(method = 'lm', aes(label = paste0('p = ', round(..p.value.., 3), " Adj. R² = ", round(..adj.r.squared.., 3))))
```
### With all
```{r}
vol = volatility(cahList, k = 5)
out = merge(round(vol, digits = 2), nwords, by="row.names", all.x=TRUE, all.y=FALSE)
out[order(out[, "V_i"]), ]
# see if there is a correlation
reg = lm(out[, 3] ~ out[, 2])
summary(reg)
plot(out[, 2], out[, 3])
abline(reg)
# Et la distrib des VI
boxplot(out[, 2])
hist(out[, 2])
```
RefcahList = list(raw3grams = CAHRaw3gr, Affs = CAHAffs, FunctWords = CAHFW, FunctLemm = CAHFL, POS3gr = CAHPOS3gr, FWPOSandAffs = CAHGlob2, Forms = CAHForms,  Lemmas = CAHLemmas, WordsLemmas = CAHWordsLemmas)
# 1. get Students analysis list
StudentsResults = replicateAnalysis(toKeep, "data/transkr_raw_char3grams.csv", "data/transkr_expanded_words.csv", "data/transkr_pos3-gr.csv", "data/transkr_lemmas.csv", functionWords, functionLemmas)
# 2. perform comparison
comp1 = compareReplications(RefcahList, StudentsResults, k = 5)
comp1
StudentsResults
StudentsResults$WordsLemmas
nrow(StudentsResults$WordsLemmas$data)
ncol(StudentsResults$WordsLemmas$data)
path_raw_char3grams
path_expanded_words
path_pos3_gr
path_lemmas
data3grams = readData(toKeep, path_raw_char3grams)
CAHRaw3gr = performAnalysis(data3grams)
dataWords = readData(toKeep, path_expanded_words)
CAHForms  = performAnalysis(dataWords)
dataAffs = countAffixes(dataWords)
CAHAffs = performAnalysis(dataAffs)
# Now, function words, a bit more annoying (should put it in readData function, yeah, right)
dataFW = relativeFreqs(dataWords)
dataFW = dataFW[functionWords,]
dataFWsave = dataFW
dataFW = normalisations(dataFW)
CAHFW = cluster::agnes(t(dataFW), metric = "manhattan", method="ward")
# back to normal. Or not.
dataPOS = readAnnotData(toKeep, path_pos3_gr)
CAHPOS3gr = performAnalysis(dataPOS)
dataLemmas = readAnnotData(toKeep, path_lemmas)
CAHLemmas = performAnalysis(dataLemmas)
# And now back to functionLemmas
dataFL = relativeFreqs(dataLemmas)
dataFL = dataFL[functionLemmas,]
dataFL = normalisations(dataFL)
CAHFL = cluster::agnes(t(dataFL), metric = "manhattan", method="ward")
# Select relevant data
dataList = list(dataAffs, dataPOS)
results = matrix(ncol = ncol(dataAffs), nrow = 0, dimnames = list(NULL, colnames(dataAffs)))
dataList
for (i in length(dataList)){
select = selection(dataList[[i]], z = 1.645)
select = select[,4]
# Normalisations
dataList[[i]] = relativeFreqs(dataList[[i]])
results = rbind(results, dataList[[i]][select,])
}
results = rbind(results, dataFWsave)
nrow(results)
length(dataLidst)
length(dataList)
results
# Select relevant data
dataList = list(dataAffs, dataPOS)
results = matrix(ncol = ncol(dataAffs), nrow = 0, dimnames = list(NULL, colnames(dataAffs)))
for (i in length(dataList)){
select = selection(dataList[[i]], z = 1.645)
select = select[,4]
# Normalisations
dataList[[i]] = relativeFreqs(dataList[[i]])
results = rbind(results, dataList[[i]][select,])
}
results
nrow(results)
rownames(results)
results
dataList[[i]][select,]
class(dataList[[i]][select,])
i
i = 1
i = 1
# Select relevant data
dataList = list(dataAffs, dataPOS)
results = matrix(ncol = ncol(dataAffs), nrow = 0, dimnames = list(NULL, colnames(dataAffs)))
select = selection(dataList[[i]], z = 1.645)
select = select[,4]
# Normalisations
dataList[[i]] = relativeFreqs(dataList[[i]])
results = rbind(results, dataList[[i]][select,])
results
i = 2
select = selection(dataList[[i]], z = 1.645)
select = select[,4]
# Normalisations
dataList[[i]] = relativeFreqs(dataList[[i]])
results = rbind(results, dataList[[i]][select,])
results
nrow(results)
rownames(results)
# Select relevant data
dataList = list(dataAffs, dataPOS)
results = matrix(ncol = ncol(dataAffs), nrow = 0, dimnames = list(NULL, colnames(dataAffs)))
for (i in length(dataList)){
select = selection(dataList[[i]], z = 1.645)
select = select[,4]
# Normalisations
dataList[[i]] = relativeFreqs(dataList[[i]])
results = rbind(results, dataList[[i]][select,])
}
length(dataList)
results
dataList[[i]]
results = matrix(ncol = ncol(dataAffs), nrow = 0, dimnames = list(NULL, colnames(dataAffs)))
for (i in 1:length(dataList)){
select = selection(dataList[[i]], z = 1.645)
select = select[,4]
# Normalisations
dataList[[i]] = relativeFreqs(dataList[[i]])
results = rbind(results, dataList[[i]][select,])
}
nrow(results)
replicateAnalysis = function(toKeep, path_raw_char3grams,
path_expanded_words, path_pos3_gr, path_lemmas,
functionWords, functionLemmas){
# toKeep: list of texts to keep for analysis
# path_raw_char3grams: path to raw_char3grams file;
# path_expanded_words, path_pos3_gr, path_lemmas: you get the idea…
# functionWords: list of functionWords
# functionLemmas: well, in fact, a list of functionLemmas (really)
data3grams = readData(toKeep, path_raw_char3grams)
CAHRaw3gr = performAnalysis(data3grams)
dataWords = readData(toKeep, path_expanded_words)
CAHForms  = performAnalysis(dataWords)
dataAffs = countAffixes(dataWords)
CAHAffs = performAnalysis(dataAffs)
# Now, function words, a bit more annoying (should put it in readData function, yeah, right)
dataFW = relativeFreqs(dataWords)
dataFW = dataFW[functionWords,]
dataFWsave = dataFW
dataFW = normalisations(dataFW)
CAHFW = cluster::agnes(t(dataFW), metric = "manhattan", method="ward")
# back to normal. Or not.
dataPOS = readAnnotData(toKeep, path_pos3_gr)
CAHPOS3gr = performAnalysis(dataPOS)
dataLemmas = readAnnotData(toKeep, path_lemmas)
CAHLemmas = performAnalysis(dataLemmas)
# And now back to functionLemmas
dataFL = relativeFreqs(dataLemmas)
dataFL = dataFL[functionLemmas,]
dataFL = normalisations(dataFL)
CAHFL = cluster::agnes(t(dataFL), metric = "manhattan", method="ward")
# And NOOOOOW: Affixes + POS 3-gr + Function words (unnorm)
# Select relevant data
dataList = list(dataAffs, dataPOS)
results = matrix(ncol = ncol(dataAffs), nrow = 0, dimnames = list(NULL, colnames(dataAffs)))
for (i in 1:length(dataList)){
select = selection(dataList[[i]], z = 1.645)
select = select[,4]
# Normalisations
dataList[[i]] = relativeFreqs(dataList[[i]])
results = rbind(results, dataList[[i]][select,])
}
results = rbind(results, dataFWsave)
rm(dataList)
dataGlob = normalisations(results)
CAHGlob2 = cluster::agnes(t(dataGlob), metric = "manhattan", method="ward")
# AND NOW: lemmas and words
# Select relevant data
dataList = list(dataLemmas, dataWords)
results = matrix(ncol = ncol(dataLemmas), nrow = 0, dimnames = list(NULL, colnames(dataLemmas)))
for (i in 1:length(dataList)){
select = selection(dataList[[i]], z = 1.645)
select = select[,4]
# Normalisations
dataList[[i]] = relativeFreqs(dataList[[i]])
results = rbind(results, dataList[[i]][select,])
}
rm(dataList)
dataWordsLemmas = normalisations(results)
CAHWordsLemmas = cluster::agnes(t(dataWordsLemmas), metric = "manhattan", method="ward")
cahList = list(raw3grams = CAHRaw3gr, Affs = CAHAffs, FunctWords = CAHFW, FunctLemm = CAHFL, POS3gr = CAHPOS3gr, FWPOSandAffs = CAHGlob2, Forms = CAHForms,  Lemmas = CAHLemmas, WordsLemmas = CAHWordsLemmas)
return(cahList)
}
# 1. get Students analysis list
StudentsResults = replicateAnalysis(toKeep, "data/transkr_raw_char3grams.csv", "data/transkr_expanded_words.csv", "data/transkr_pos3-gr.csv", "data/transkr_lemmas.csv", functionWords, functionLemmas)
# 2. perform comparison
comp1 = compareReplications(RefcahList, StudentsResults, k = 5)
comp1
read.csv("data/transkr_student_lemmas.csv", sep=";")
# 1. get Students analysis list
StudentsResults = replicateAnalysis(toKeep, "data/transkr_student_raw_char3grams.csv", "data/transkr_student_expanded_words.csv", "data/transkr_student_pos3-gr.csv", "data/transkr_student_lemmas.csv", functionWords, functionLemmas)
path_raw_char3grams = "data/transkr_student_raw_char3grams.csv"
data3grams = readData(toKeep, path_raw_char3grams)
path = path_raw_char3grams
# toKeep: texts to keep for analysis
# path: path to data
# return: data…
data = read.csv(path, header = TRUE, row.names = 1)
data = t(data)
data = data[, toKeep]
colnames(data)
rownames(data)
path
# toKeep: texts to keep for analysis
# path: path to data
# return: data…
data = read.csv(path, header = TRUE, row.names = 1)
data
View(data)
# 3. get Kraken analysis list
KrakenResults = replicateAnalysis(toKeep, "data/kraken_nospace_raw_char3grams.csv", "data/kraken_nospace_expanded_words.csv", "data/kraken_nospace_pos3-gr.csv", "data/kraken_nospace_lemmas.csv", functionWords, functionLemmas)
# 4. perform comparison
comp2 = compareReplications(RefcahList, KrakenResults, k = 5)
comp2
round(comp2, 2)
data = stylo::load.corpus.and.parse(corpus.dir = "../dh-meier-data/output/transkribus-etudiants/raw/", features = "c", ngram.size = 3, preserve.case = FALSE)
data = stylo::make.table.of.frequencies(corpus = data, features = unique(sort(unlist(data))), relative = FALSE)
write.csv(as.matrix(data), "data/transkr_student_raw_char3grams.csv")
# 1. get Students analysis list
StudentsResults = replicateAnalysis(toKeep, "data/transkr_student_raw_char3grams.csv", "data/transkr_student_expanded_words.csv", "data/transkr_student_pos3-gr.csv", "data/transkr_student_lemmas.csv", functionWords, functionLemmas)
data
# toKeep: texts to keep for analysis
# path: path to data
# return: data…
data = read.csv(path, header = TRUE, row.names = 1)
data = t(data)
data = data[, toKeep]
colnames(data)
colnames(data)[!colnames(data) %in% toKeep]
toKeep[!toKeep %in% colnames(data)]
colnames(data)
data = stylo::load.corpus.and.parse(corpus.dir = "../dh-meier-data/output/transkribus-etudiants/raw/", features = "c", ngram.size = 3, preserve.case = FALSE)
data = stylo::make.table.of.frequencies(corpus = data, features = unique(sort(unlist(data))), relative = FALSE)
write.csv(as.matrix(data), "data/transkr_student_raw_char3grams.csv")
# 1. get Students analysis list
StudentsResults = replicateAnalysis(toKeep, "data/transkr_student_raw_char3grams.csv", "data/transkr_student_expanded_words.csv", "data/transkr_student_pos3-gr.csv", "data/transkr_student_lemmas.csv", functionWords, functionLemmas)
# toKeep: texts to keep for analysis
# path: path to data
# return: data…
data = read.csv(path, header = TRUE, row.names = 1)
data = t(data)
data = data[, toKeep]
data = data[rowSums(data) > 0, ]
data
data3grams = readData(toKeep, path_raw_char3grams)
CAHRaw3gr = performAnalysis(data3grams)
# data: well, … data.
# returns: a HC (agnes object)
# Selection based on Moisl 2011
select = selection(data, z = 1.645)
select
select = select[,4]
# Normalisations
data = relativeFreqs(data)
data = data[select,]
data
nrow(data)
# toKeep: texts to keep for analysis
# path: path to data
# return: data…
data = read.csv(path, header = TRUE, row.names = 1)
data = t(data)
data = data[, toKeep]
data = data[rowSums(data) > 0, ]
colSums(data)
plot(colSums(data))
boxplot(colSums(data))
head(colSums(data))
tail(colSums(data))
